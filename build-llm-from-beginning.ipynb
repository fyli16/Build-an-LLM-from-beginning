{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a720a5",
   "metadata": {},
   "source": [
    "# load raw text dataset and split the text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84a423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "  raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98999bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb8fe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\\n\\n\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Thwing--his last Chicago sitter--deploring his unaccountable abdication. \"Of course it\\'s going to send the value of my picture \\'way up; but I don\\'t think of that, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing\\'s lips, multiplied its _rs_ as though they were reflected in an endless vista of mirrors. And it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia Croft, at the last Grafton Gallery show, stopped me before Gisburn\\'s \"Moon-dancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\\n\\nWell!--even through th'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:1000]  # Display the first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1c728a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20479"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text)  # Display the length of the text to confirm it has been loaded correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9fc2d",
   "metadata": {},
   "source": [
    "Split the text into tokens based on punctuation and whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dba80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# text = 'Hello, world. This, is a test.'\n",
    "# result = re.split(r'(\\s)', text)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c48fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "# print(result)\n",
    "preprocessed = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba9cf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed)  # Display the length of the preprocessed text to confirm it has been processed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dbc4d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[:10]  # Display the first 10 tokens to verify preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9a243",
   "metadata": {},
   "source": [
    "# converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329ff86",
   "metadata": {},
   "source": [
    "first build a vocabulary from the preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10cdebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(preprocessed)  # Display the unique tokens in the preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a792db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(all_words)\n",
    "print(vocab_size)  # Display the size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e161fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25c1faec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('off', 0)\n",
      "('chucked', 1)\n",
      "('get', 2)\n",
      "('Carlo', 3)\n",
      "('tribute', 4)\n",
      "('least', 5)\n",
      "('course', 6)\n",
      "('an', 7)\n",
      "('etching', 8)\n",
      "('from', 9)\n"
     ]
    }
   ],
   "source": [
    "for index, item in enumerate(list(vocab.items())[:10]):\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d23ad3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab # {str:int}\n",
    "    self.int_to_str = {i:s for s, i in vocab.items()} # {int:str}\n",
    "  \n",
    "  def encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    \n",
    "    preprocessed = [\n",
    "      item.strip() for item in preprocessed if item.strip()\n",
    "    ] # Remove empty strings\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids \n",
    "\n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    # replace spaces before the specified punctuations\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6b83fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df2132fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[960, 208, 412, 11, 680, 35, 59, 750, 622, 875, 438, 622, 960, 774, 744, 13, 994, 536, 853, 1049, 744]\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "    Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9497e1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode \n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048bc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7c3af97",
   "metadata": {},
   "source": [
    "# adding special context tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75dbf4d",
   "metadata": {},
   "source": [
    "the vocabulary is limited, e.g. it does not contain 'hello' and cannot tokenize the following simple sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d7041a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# e.g. Hello is not in the vocabulary \u001b[39;00m\n\u001b[32m      2\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea. is this-- a test?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tokenizer.decode(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      9\u001b[39m preprocessed = [\n\u001b[32m     10\u001b[39m   item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     11\u001b[39m ] \u001b[38;5;66;03m# Remove empty strings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# e.g. Hello is not in the vocabulary \n",
    "text = \"Hello, do you like tea. is this-- a test?\"\n",
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee5d12",
   "metadata": {},
   "source": [
    "expand the vocabulary with special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|enoftext|>\", \"<|unk|>\"]) \n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8d125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items()) # it now has two more tokens \"<|enoftext|>\", \"<|unk|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c0b51c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|enoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc735cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab # {str:int}\n",
    "    self.int_to_str = {i:s for s, i in vocab.items()} # {int:str}\n",
    "  \n",
    "  def encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    \n",
    "    preprocessed = [\n",
    "      item.strip() for item in preprocessed if item.strip()\n",
    "    ] # Remove empty strings\n",
    "    preprocessed = [\n",
    "      item if item in self.str_to_int\n",
    "      else \"<|unk|>\" for item in preprocessed\n",
    "    ]\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids \n",
    "\n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    # replace spaces before the specified punctuations\n",
    "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e012c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab338b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b2c1b",
   "metadata": {},
   "source": [
    "# byte pair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4e9f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
